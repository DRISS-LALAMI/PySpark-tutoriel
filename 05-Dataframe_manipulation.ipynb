{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "88f648b8-0f63-4edd-a421-4d6fd0234a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "# Add PySpark and Python paths manually (adjust paths as needed)\n",
    "spark_home = os.environ.get('SPARK_HOME', None)  # Optional: Set if using standalone Spark\n",
    "python_path = os.path.join(spark_home, 'python') if spark_home else ''\n",
    "sys.path.insert(0, python_path)\n",
    "sys.path.insert(0, os.path.join(python_path, 'lib', 'py4j-0.10.9.7-src.zip'))  # Match your Py4J version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d26794d1-0ac9-4049-9f41-22bf678cf6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['SPARK_HOME'] = \"/Applications/spark-3.5.5-bin-hadoop3\"\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = 'jupyter'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON_OPTS'] = 'lab'\n",
    "os.environ['PYSPARK_PYTHON'] = 'python'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7206aa5e-eaec-46d2-a5e2-f4df54b2a47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"CREATE-DATAFRAME\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b794f93a-3b0d-43f9-849a-8dc7ffe092b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DateTime,Temperature,Humidity,Wind Speed,general diffuse flows,diffuse flows,Zone 1,Zone 2  ,Zone 3  \n",
      "01-01-2017 00:00,6.559,73.8,0.083,0.051,0.119,34055.6962,16128.87538,20240.96386\n",
      "01-01-2017 00:10,6.414,74.5,0.083,0.07,0.085,29814.68354,19375.07599,20131.08434\n",
      "01-01-2017 00:20,6.313,74.5,0.08,0.062,0.1,29128.10127,19006.68693,19668.43373\n",
      "01-01-2017 00:30,6.121,75,0.083,0.091,0.096,28228.86076,18361.09422,18899.27711\n",
      "01-01-2017 00:40,5.921,75.7,0.081,0.048,0.085,27335.6962,17872.34043,18442.40964\n",
      "01-01-2017 00:50,5.853,76.9,0.081,0.059,0.108,26624.81013,17416.41337,18130.12048\n",
      "01-01-2017 01:00,5.641,77.7,0.08,0.048,0.096,25998.98734,16993.31307,17945.06024\n",
      "01-01-2017 01:10,5.496,78.2,0.085,0.055,0.093,25446.07595,16661.39818,17459.27711\n",
      "01-01-2017 01:20,5.678,78.1,0.081,0.066,0.141,24777.72152,16227.35562,17025.54217\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "head -10 ./data_csv/power_consumption.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "22772553-4358-418d-9c6d-de73f5e66ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV file into DataFrame\n",
    "csv_file_path = \"./data_files/power_consumption.csv\"\n",
    "df = spark.read.csv(csv_file_path, header= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ca4d344-d7f3-4ea3-9a81-21075bd5f6a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DateTime: string (nullable = true)\n",
      " |-- Temperature: string (nullable = true)\n",
      " |-- Humidity: string (nullable = true)\n",
      " |-- Wind Speed: string (nullable = true)\n",
      " |-- general diffuse flows: string (nullable = true)\n",
      " |-- diffuse flows: string (nullable = true)\n",
      " |-- Zone 1: string (nullable = true)\n",
      " |-- Zone 2  : string (nullable = true)\n",
      " |-- Zone 3  : string (nullable = true)\n",
      "\n",
      "+----------------+-----------+--------+----------+---------------------+-------------+-----------+-----------+-----------+\n",
      "|        DateTime|Temperature|Humidity|Wind Speed|general diffuse flows|diffuse flows|     Zone 1|   Zone 2  |   Zone 3  |\n",
      "+----------------+-----------+--------+----------+---------------------+-------------+-----------+-----------+-----------+\n",
      "|01-01-2017 00:00|      6.559|    73.8|     0.083|                0.051|        0.119| 34055.6962|16128.87538|20240.96386|\n",
      "|01-01-2017 00:10|      6.414|    74.5|     0.083|                 0.07|        0.085|29814.68354|19375.07599|20131.08434|\n",
      "|01-01-2017 00:20|      6.313|    74.5|      0.08|                0.062|          0.1|29128.10127|19006.68693|19668.43373|\n",
      "|01-01-2017 00:30|      6.121|      75|     0.083|                0.091|        0.096|28228.86076|18361.09422|18899.27711|\n",
      "|01-01-2017 00:40|      5.921|    75.7|     0.081|                0.048|        0.085| 27335.6962|17872.34043|18442.40964|\n",
      "+----------------+-----------+--------+----------+---------------------+-------------+-----------+-----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display the schema of DataFrame\n",
    "df.printSchema()\n",
    "\n",
    "# Display content of DataFrame\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95af3c91-3a45-4e79-aa4e-a91bba4b95fb",
   "metadata": {},
   "source": [
    "# Read CSV with an explicit schema definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3202809c-252b-4c27-a120-3c8646f0002b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField, StructType, StringType, IntegerType, DoubleType, DateType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "980e51c5-2a75-412c-a19d-a0e63964427a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the schema\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(name= \"DateTime\", dataType= StringType(), nullable= True),\n",
    "    StructField(name= \"Temperature\", dataType= DoubleType(), nullable = True), \n",
    "    StructField(name= \"Humidity\", dataType= DoubleType(), nullable = True), \n",
    "    StructField(name= \"Wind Speed\", dataType= DoubleType(), nullable = True), \n",
    "    StructField(name= \"general diffuse\", dataType= DoubleType(), nullable = True), \n",
    "    StructField(name= \"diffuse flows\", dataType= DoubleType(), nullable = True), \n",
    "    StructField(name= \"Zone 1\", dataType= DoubleType(), nullable = True), \n",
    "    StructField(name= \"Zone 2\", dataType= DoubleType(), nullable = True), \n",
    "    StructField(name= \"Zone 3\", dataType= DoubleType(), nullable = True)\n",
    "])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "65bdf71a-39e6-4a35-aa9e-ccc5a580607d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading from csv file using schema\n",
    "df = spark.read.csv(csv_file_path, header= True, schema= schema )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "715bcca5-4dfb-4591-b0d8-0f8103a6e289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DateTime: string (nullable = true)\n",
      " |-- Temperature: double (nullable = true)\n",
      " |-- Humidity: double (nullable = true)\n",
      " |-- Wind Speed: double (nullable = true)\n",
      " |-- general diffuse: double (nullable = true)\n",
      " |-- diffuse flows: double (nullable = true)\n",
      " |-- Zone 1: double (nullable = true)\n",
      " |-- Zone 2: double (nullable = true)\n",
      " |-- Zone 3: double (nullable = true)\n",
      "\n",
      "+----------------+-----------+--------+----------+---------------+-------------+-----------+-----------+-----------+\n",
      "|        DateTime|Temperature|Humidity|Wind Speed|general diffuse|diffuse flows|     Zone 1|     Zone 2|     Zone 3|\n",
      "+----------------+-----------+--------+----------+---------------+-------------+-----------+-----------+-----------+\n",
      "|01-01-2017 00:00|      6.559|    73.8|     0.083|          0.051|        0.119| 34055.6962|16128.87538|20240.96386|\n",
      "|01-01-2017 00:10|      6.414|    74.5|     0.083|           0.07|        0.085|29814.68354|19375.07599|20131.08434|\n",
      "|01-01-2017 00:20|      6.313|    74.5|      0.08|          0.062|          0.1|29128.10127|19006.68693|19668.43373|\n",
      "+----------------+-----------+--------+----------+---------------+-------------+-----------+-----------+-----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/15 11:24:00 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DateTime, Temperature, Humidity, Wind Speed, general diffuse flows, diffuse flows, Zone 1, Zone 2  , Zone 3  \n",
      " Schema: DateTime, Temperature, Humidity, Wind Speed, general diffuse, diffuse flows, Zone 1, Zone 2, Zone 3\n",
      "Expected: general diffuse but found: general diffuse flows\n",
      "CSV file: file:///Users/apple/projects/data_projects/data_ing/PySpark-tutoriel/data_csv/power_consumption.csv\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n",
    "\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878b45b8-46a1-4614-b2c4-aa153061f27f",
   "metadata": {},
   "source": [
    "# Read csv with inferSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a594192b-cb69-4f00-8412-2289581d31d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark guess the schema automatically when using inferSchema\n",
    "\n",
    "# reading from csv file using schema\n",
    "csv_file_path = \"./data_files/power_consumption.csv\"\n",
    "df = spark.read.csv(csv_file_path, header= True, inferSchema= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc05d101-19f0-4243-af13-ef8b11fb4520",
   "metadata": {},
   "source": [
    "df.printSchema()\n",
    "\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b7f1a1-ebf7-4362-9543-e74346cb0ee1",
   "metadata": {},
   "source": [
    "# Read JSON file into DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2fe56031-90a2-49cf-9976-3b25b9a216f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"name\":\"Michael\",\"age\":45}\n",
      "{\"name\":\"Andy\", \"age\":30}\n",
      "{\"name\":\"Justin\", \"age\":19}\n",
      "{\"name\":\"Driss\",\"age\":25}\n",
      "{\"name\":\"Ali\", \"age\":20}\n",
      "{\"name\":\"Ahmed\", \"age\":22}\n",
      "{\"name\":\"Amine\",\"age\":17}\n",
      "{\"name\":\"Azzedine\", \"age\":27}\n",
      "{\"name\":\"Aziz\", \"age\":55}\n",
      "{\"name\":\"Mohcine\",\"age\":18}\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "head -10 ./data_files/people.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2cd25ed4-1c5f-4c98-88a3-c23dc4e7fc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file_path = \"./data_files/people.json\"\n",
    "\n",
    "df =spark.read.json(json_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2ecc54d1-6e9e-440b-ac2c-2adb9df30d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n",
      "+---+-------+\n",
      "|age|   name|\n",
      "+---+-------+\n",
      "| 45|Michael|\n",
      "| 30|   Andy|\n",
      "| 19| Justin|\n",
      "| 25|  Driss|\n",
      "| 20|    Ali|\n",
      "+---+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbac3a6-c920-478d-98ee-e7ce8fbb00a6",
   "metadata": {},
   "source": [
    "# Write dataframe into parquet file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "79e70e84-3efa-4d40-ae34-617797da56f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_file_path = \"./data_files/people.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e145e040-e60d-48dd-96c4-6a0a75d81d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df.write.parquet(parquet_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b36912-bcd2-473a-95ba-37057317262d",
   "metadata": {},
   "source": [
    "# Read parquet file into dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5fead01a-10fb-44ca-a37c-92035450fd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(parquet_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "af49ffd1-c27f-4926-953a-060d224914e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "|age|   name|\n",
      "+---+-------+\n",
      "| 45|Michael|\n",
      "| 30|   Andy|\n",
      "| 19| Justin|\n",
      "| 25|  Driss|\n",
      "| 20|    Ali|\n",
      "| 22|  Ahmed|\n",
      "| 17|  Amine|\n",
      "+---+-------+\n",
      "only showing top 7 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema\n",
    "\n",
    "df.show(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "036f198e-303a-44d0-8d40-ce0b2d72346c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0975d74-9fd6-4fc5-85c2-102d6563b768",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
